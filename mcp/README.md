# Shelve MCP Server

Serveur MCP (Model Context Protocol) pour le syst√®me d'archivage Shelve avec int√©gration IA Ollama.

## üöÄ Fonctionnalit√©s

- **R√©sum√© automatique** : G√©n√©ration de r√©sum√©s de documents avec diff√©rents niveaux de d√©tail
- **Extraction de mots-cl√©s** : Identification automatique des termes cl√©s avec scores de pertinence
- **Reformulation de titres** : Am√©lioration et optimisation des titres de documents
- **Analyse de contenu** : Analyse structurelle, sentimentale et technique des documents
- **Traitement complet** : Combinaison de tous les traitements en une seule requ√™te
- **API REST** : Interface HTTP simple et document√©e
- **Gestion d'erreurs** : Syst√®me robuste de gestion et logging des erreurs
- **Health checks** : Surveillance de l'√©tat du syst√®me et des services

## üìÅ Structure du projet

```
mcp/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ config/           # Configuration centralis√©e
‚îÇ   ‚îú‚îÄ‚îÄ controllers/      # Contr√¥leurs REST
‚îÇ   ‚îú‚îÄ‚îÄ services/         # Logique m√©tier
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ai/          # Services IA (Ollama)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ database/    # Services base de donn√©es
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ processing/  # Services de traitement
‚îÇ   ‚îú‚îÄ‚îÄ middleware/       # Middlewares Express
‚îÇ   ‚îú‚îÄ‚îÄ routes/          # D√©finition des routes
‚îÇ   ‚îú‚îÄ‚îÄ utils/           # Utilitaires
‚îÇ   ‚îî‚îÄ‚îÄ models/          # Mod√®les de donn√©es
‚îú‚îÄ‚îÄ templates/           # Templates de prompts
‚îú‚îÄ‚îÄ tests/              # Tests unitaires et d'int√©gration
‚îú‚îÄ‚îÄ docs/               # Documentation
‚îú‚îÄ‚îÄ scripts/            # Scripts utilitaires
‚îî‚îÄ‚îÄ logs/               # Fichiers de logs
```

## ‚öôÔ∏è Installation

### Pr√©requis

- Node.js >= 16.0.0
- npm >= 8.0.0
- Ollama install√© et fonctionnel
- MySQL/MariaDB (optionnel)

### Installation des d√©pendances

```bash
cd mcp
npm install
```

### Configuration

1. Copier le fichier d'environnement :
```bash
copy .env.example .env
```

2. Modifier les variables d'environnement dans `.env` :
```env
# Configuration du serveur
PORT=3001
NODE_ENV=development
LOG_LEVEL=debug

# Configuration Ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_DEFAULT_MODEL=llama3.2

# Configuration base de donn√©es (optionnel)
DB_HOST=localhost
DB_DATABASE=shelve
DB_USERNAME=root
DB_PASSWORD=
```

### D√©marrage

```bash
# Mode d√©veloppement
npm run dev

# Mode production
npm start
```

Le serveur sera accessible sur `http://localhost:3001`

## üìö Utilisation de l'API

### Endpoints principaux

#### R√©sum√© de contenu
```http
POST /api/records/summarize
Content-Type: application/json

{
  "content": "Votre texte √† r√©sumer...",
  "model": "llama3.2",
  "options": {
    "maxLength": 200,
    "type": "basic",
    "language": "fr"
  }
}
```

#### Extraction de mots-cl√©s
```http
POST /api/records/keywords
Content-Type: application/json

{
  "content": "Votre texte...",
  "count": 10,
  "model": "llama3.2"
}
```

#### Reformulation de titre
```http
POST /api/records/reformulate-title
Content-Type: application/json

{
  "title": "Titre actuel",
  "content": "Contenu du document...",
  "model": "llama3.2"
}
```

#### Analyse de contenu
```http
POST /api/records/analyze
Content-Type: application/json

{
  "content": "Votre texte...",
  "type": "general",
  "model": "llama3.2"
}
```

#### Traitement complet
```http
POST /api/records/process-complete
Content-Type: application/json

{
  "content": "Votre texte...",
  "model": "llama3.2",
  "options": {
    "summary": {"maxLength": 200},
    "keywordCount": 10,
    "analysisType": "general"
  }
}
```

### Endpoints utilitaires

- `GET /api/health` - √âtat du syst√®me
- `GET /api/models` - Liste des mod√®les disponibles
- `GET /api/settings` - Configuration du serveur

## üß™ Tests

```bash
# Ex√©cuter tous les tests
npm test

# Tests en mode watch
npm run test:watch

# Coverage des tests
npm run test:coverage
```

## üìä Monitoring et Logs

Les logs sont automatiquement g√©n√©r√©s dans le dossier `logs/` :

- `app.log` - Tous les logs de l'application
- `error.log` - Logs d'erreurs uniquement
- `access.log` - Logs d'acc√®s HTTP

Niveaux de log : `error`, `warn`, `info`, `debug`

## üîß Configuration avanc√©e

### Templates de prompts

Les templates sont stock√©s dans `templates/` et utilisent un syst√®me de variables :

```text
Vous √™tes un expert en {{task}}. 
Analysez le contenu suivant : {{content}}
```

### Mod√®les Ollama

Configuration des mod√®les par t√¢che dans `.env` :

```env
OLLAMA_SUMMARY_MODEL=llama3.2
OLLAMA_KEYWORDS_MODEL=llama3.2
OLLAMA_TITLE_MODEL=llama3.2
OLLAMA_ANALYSIS_MODEL=llama3.2
```

### Rate Limiting

```env
RATE_LIMIT_WINDOW=900000  # 15 minutes
RATE_LIMIT_MAX=100        # 100 requ√™tes par fen√™tre
```

## üöÄ D√©ploiement

### Avec Docker (√† venir)

```bash
docker build -t shelve-mcp .
docker run -p 3001:3001 shelve-mcp
```

### Avec PM2

```bash
npm install -g pm2
pm2 start src/index.js --name "shelve-mcp"
```

## ü§ù Contribution

1. Forkez le projet
2. Cr√©ez votre branche de fonctionnalit√©
3. Committez vos changements
4. Poussez vers la branche
5. Ouvrez une Pull Request

## üìã Scripts disponibles

- `npm start` - D√©marrage en production
- `npm run dev` - D√©marrage en d√©veloppement avec nodemon
- `npm test` - Ex√©cution des tests
- `npm run lint` - V√©rification du code avec ESLint
- `npm run lint:fix` - Correction automatique des erreurs ESLint
- `npm run docs` - G√©n√©ration de la documentation JSDoc

## üêõ R√©solution de probl√®mes

### Ollama indisponible
- V√©rifiez qu'Ollama est d√©marr√© : `ollama serve`
- V√©rifiez l'URL de connexion dans `.env`

### Erreurs de mod√®le
- Listez les mod√®les disponibles : `ollama list`
- T√©l√©chargez un mod√®le : `ollama pull llama3.2`

### Probl√®mes de performance
- Ajustez `OLLAMA_MAX_CONCURRENT` dans `.env`
- R√©duisez `OLLAMA_MAX_TOKENS` pour les requ√™tes plus rapides

## üìû Support

Pour signaler un bug ou demander une fonctionnalit√©, veuillez ouvrir une issue sur le d√©p√¥t GitHub.

## üìÑ Licence

Ce projet est sous licence MIT. Voir le fichier [LICENSE](LICENSE) pour plus de d√©tails.

## üôè Remerciements

- [Ollama](https://ollama.ai/) pour l'interface IA
- [Express.js](https://expressjs.com/) pour le framework web
- [Winston](https://github.com/winstonjs/winston) pour le logging
