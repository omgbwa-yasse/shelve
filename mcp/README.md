# MCP Shelve - Serveur de Traitement de Documents d'Archives

## Description

Serveur MCP (Model Context Protocol) sp√©cialis√© dans le traitement intelligent de documents d'archives avec int√©gration Ollama pour l'IA g√©n√©rative. Con√ßu sp√©cifiquement pour respecter les normes fran√ßaises de description archivistique.

## üöÄ D√©marrage rapide

1. **Installation des d√©pendances**
   ```bash
   cd mcp
   npm install
   ```

2. **Configuration**
   ```bash
   cp .env.example .env
   # √âditer .env avec vos param√®tres
   ```

3. **D√©marrage d'Ollama**
   ```bash
   ollama serve
   ollama pull llama3.2
   ```

4. **Lancement du serveur**
   ```bash
   npm run dev
   ```

5. **Test**
   ```bash
   curl http://localhost:3001/api/health
   ```

## üéØ Fonctionnalit√©s principales

### Reformulation de titres archivistiques
- **Standard** : Am√©lioration g√©n√©rale de titres
- **Archivistique** : Respect des normes fran√ßaises (point-tiret, structure hi√©rarchique)
- **G√©n√©ration** : Cr√©ation de titres complets √† partir du contenu

### Traitement intelligent
- Extraction de mots-cl√©s th√©matiques
- G√©n√©ration de r√©sum√©s structur√©s  
- Analyse s√©mantique approfondie
- Validation selon normes archivistiques

### API REST compl√®te
- Endpoints sp√©cialis√©s par type de traitement
- Validation robuste avec Joi
- Gestion d'erreurs centralis√©e
- Monitoring et logging

## üìö Documentation

| Guide | Description |
|-------|-------------|
| [üöÄ D√©marrage rapide](docs/QUICK_START.md) | Installation et premiers tests |
| [üìñ Reformulation de titres](docs/TITLE_REFORMULATION.md) | Exemples et normes archivistiques |
| [üîå API Reference](docs/API.md) | Documentation compl√®te des endpoints |
| [üèóÔ∏è Architecture](docs/ARCHITECTURE.md) | Structure technique d√©taill√©e |

## üîß Technologies

- **Backend :** Node.js 16+, Express.js
- **IA :** Ollama (llama3.2) avec gestion des erreurs
- **Base de donn√©es :** MySQL/MariaDB avec Knex.js ORM
- **Logging :** Winston (app.log, error.log)
- **Validation :** Joi avec sch√©mas stricts
- **Tests :** Jest avec couverture compl√®te

- Node.js >= 16.0.0
- npm >= 8.0.0
- Ollama install√© et fonctionnel
- MySQL/MariaDB (optionnel)

### Installation des d√©pendances

```bash
cd mcp
npm install
```

### Configuration

1. Copier le fichier d'environnement :
```bash
copy .env.example .env
```

2. Modifier les variables d'environnement dans `.env` :
```env
# Configuration du serveur
PORT=3001
NODE_ENV=development
LOG_LEVEL=debug

# Configuration Ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_DEFAULT_MODEL=llama3.2

# Configuration base de donn√©es (optionnel)
DB_HOST=localhost
DB_DATABASE=shelve
DB_USERNAME=root
DB_PASSWORD=
```

### D√©marrage

```bash
# Mode d√©veloppement
npm run dev

# Mode production
npm start
```

Le serveur sera accessible sur `http://localhost:3001`

## üìö Utilisation de l'API

### Endpoints principaux

#### R√©sum√© de contenu
```http
POST /api/records/summarize
Content-Type: application/json

{
  "content": "Votre texte √† r√©sumer...",
  "model": "llama3.2",
  "options": {
    "maxLength": 200,
    "type": "basic",
    "language": "fr"
  }
}
```

#### Extraction de mots-cl√©s
```http
POST /api/records/keywords
Content-Type: application/json

{
  "content": "Votre texte...",
  "count": 10,
  "model": "llama3.2"
}
```

#### Reformulation de titre
```http
POST /api/records/reformulate-title
Content-Type: application/json

{
  "title": "Titre actuel",
  "content": "Contenu du document...",
  "model": "llama3.2"
}
```

#### Analyse de contenu
```http
POST /api/records/analyze
Content-Type: application/json

{
  "content": "Votre texte...",
  "type": "general",
  "model": "llama3.2"
}
```

#### Traitement complet
```http
POST /api/records/process-complete
Content-Type: application/json

{
  "content": "Votre texte...",
  "model": "llama3.2",
  "options": {
    "summary": {"maxLength": 200},
    "keywordCount": 10,
    "analysisType": "general"
  }
}
```

### Endpoints utilitaires

- `GET /api/health` - √âtat du syst√®me
- `GET /api/models` - Liste des mod√®les disponibles
- `GET /api/settings` - Configuration du serveur

## üß™ Tests

```bash
# Ex√©cuter tous les tests
npm test

# Tests en mode watch
npm run test:watch

# Coverage des tests
npm run test:coverage
```

## üìä Monitoring et Logs

Les logs sont automatiquement g√©n√©r√©s dans le dossier `logs/` :

- `app.log` - Tous les logs de l'application
- `error.log` - Logs d'erreurs uniquement
- `access.log` - Logs d'acc√®s HTTP

Niveaux de log : `error`, `warn`, `info`, `debug`

## üîß Configuration avanc√©e

### Templates de prompts

Les templates sont stock√©s dans `templates/` et utilisent un syst√®me de variables :

```text
Vous √™tes un expert en {{task}}. 
Analysez le contenu suivant : {{content}}
```

### Mod√®les Ollama

Configuration des mod√®les par t√¢che dans `.env` :

```env
OLLAMA_SUMMARY_MODEL=llama3.2
OLLAMA_KEYWORDS_MODEL=llama3.2
OLLAMA_TITLE_MODEL=llama3.2
OLLAMA_ANALYSIS_MODEL=llama3.2
```

### Rate Limiting

```env
RATE_LIMIT_WINDOW=900000  # 15 minutes
RATE_LIMIT_MAX=100        # 100 requ√™tes par fen√™tre
```

## üöÄ D√©ploiement

### Avec Docker (√† venir)

```bash
docker build -t shelve-mcp .
docker run -p 3001:3001 shelve-mcp
```

### Avec PM2

```bash
npm install -g pm2
pm2 start src/index.js --name "shelve-mcp"
```

## ü§ù Contribution

1. Forkez le projet
2. Cr√©ez votre branche de fonctionnalit√©
3. Committez vos changements
4. Poussez vers la branche
5. Ouvrez une Pull Request

## üìã Scripts disponibles

- `npm start` - D√©marrage en production
- `npm run dev` - D√©marrage en d√©veloppement avec nodemon
- `npm test` - Ex√©cution des tests
- `npm run lint` - V√©rification du code avec ESLint
- `npm run lint:fix` - Correction automatique des erreurs ESLint
- `npm run docs` - G√©n√©ration de la documentation JSDoc

## üêõ R√©solution de probl√®mes

### Ollama indisponible
- V√©rifiez qu'Ollama est d√©marr√© : `ollama serve`
- V√©rifiez l'URL de connexion dans `.env`

### Erreurs de mod√®le
- Listez les mod√®les disponibles : `ollama list`
- T√©l√©chargez un mod√®le : `ollama pull llama3.2`

### Probl√®mes de performance
- Ajustez `OLLAMA_MAX_CONCURRENT` dans `.env`
- R√©duisez `OLLAMA_MAX_TOKENS` pour les requ√™tes plus rapides

## üìû Support

Pour signaler un bug ou demander une fonctionnalit√©, veuillez ouvrir une issue sur le d√©p√¥t GitHub.

## üìÑ Licence

Ce projet est sous licence MIT. Voir le fichier [LICENSE](LICENSE) pour plus de d√©tails.

## üôè Remerciements

- [Ollama](https://ollama.ai/) pour l'interface IA
- [Express.js](https://expressjs.com/) pour le framework web
- [Winston](https://github.com/winstonjs/winston) pour le logging
